{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "sap5VQWUwG9C"
      },
      "source": [
        "# Google Colab Notebook 2: Load and Demonstrate Model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ngAi2o73wG9D"
      },
      "outputs": [],
      "source": [
        "# Install necessary libraries\n",
        "!pip install transformers accelerate"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "SRuVqpFCwG9D"
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "from transformers import GPT2LMHeadModel, GPT2Tokenizer"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "znF2MXn_wG9E"
      },
      "outputs": [],
      "source": [
        "# Load the model\n",
        "model_name = \"final_model\"  # Change this to the appropriate saved model directory\n",
        "model = GPT2LMHeadModel.from_pretrained(model_name)\n",
        "tokenizer = GPT2Tokenizer.from_pretrained(model_name)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "b_gWsqKrwG9E"
      },
      "outputs": [],
      "source": [
        "# Function to generate responses\n",
        "def generate_response(query):\n",
        "    inputs = tokenizer(query, return_tensors='pt')\n",
        "    outputs = model.generate(inputs.input_ids, max_length=50)\n",
        "    response = tokenizer.decode(outputs[0], skip_special_tokens=True)\n",
        "    return response"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "xYc86bspwG9E"
      },
      "outputs": [],
      "source": [
        "# Interactive query input\n",
        "while True:\n",
        "    query = input(\"Enter your query (or type 'exit' to quit): \")\n",
        "    if query.lower() == 'exit':\n",
        "        break\n",
        "    response = generate_response(query)\n",
        "    print(f\"Response: {response}\")"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.7.12"
    },
    "colab": {
      "provenance": []
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}